{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:03:20.499078Z",
     "start_time": "2019-03-24T17:03:19.704197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:03:22.286205Z",
     "start_time": "2019-03-24T17:03:22.279223Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:03:23.583177Z",
     "start_time": "2019-03-24T17:03:23.578563Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:03:38.517679Z",
     "start_time": "2019-03-24T17:03:26.613195Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../[Data] Sarcasm-Detection/reddit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:03:38.869088Z",
     "start_time": "2019-03-24T17:03:38.847998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010826, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:03:39.126489Z",
     "start_time": "2019-03-24T17:03:39.029208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd prefer is she lived in NC as well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams more than east teams right?</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 seed) did not even carry a good enough record to make the playoffs in the east last year.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since Gronk's announcement this afternoon, the Vegas line has moved to patriots -1</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york nigga\" ones are.</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for that. It was made by our boy EASports_MUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0  0       \n",
       "1  0       \n",
       "2  0       \n",
       "3  0       \n",
       "4  0       \n",
       "\n",
       "                                                                                                                     comment  \\\n",
       "0  NC and NH.                                                                                                                  \n",
       "1  You do know west teams play against west teams more than east teams right?                                                  \n",
       "2  They were underdogs earlier today, but since Gronk's announcement this afternoon, the Vegas line has moved to patriots -1   \n",
       "3  This meme isn't funny none of the \"new york nigga\" ones are.                                                                \n",
       "4  I could use one of those tools.                                                                                             \n",
       "\n",
       "      author           subreddit  score  ups  downs     date  \\\n",
       "0  Trumpbart  politics            2     -1   -1      2016-10   \n",
       "1  Shbshb906  nba                -4     -1   -1      2016-11   \n",
       "2  Creepeth   nfl                 3      3    0      2016-09   \n",
       "3  icebrotha  BlackPeopleTwitter -8     -1   -1      2016-10   \n",
       "4  cush2push  MaddenUltimateTeam  6     -1   -1      2016-12   \n",
       "\n",
       "           created_utc  \\\n",
       "0  2016-10-16 23:55:23   \n",
       "1  2016-11-01 00:24:10   \n",
       "2  2016-09-22 21:45:37   \n",
       "3  2016-10-18 21:03:47   \n",
       "4  2016-12-30 17:00:13   \n",
       "\n",
       "                                                                                                                           parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd prefer is she lived in NC as well.                                                        \n",
       "1  The blazers and Mavericks (The wests 5 and 6 seed) did not even carry a good enough record to make the playoffs in the east last year.  \n",
       "2  They're favored to win.                                                                                                                 \n",
       "3  deadass don't kill my buzz                                                                                                              \n",
       "4  Yep can confirm I saw the tool they use for that. It was made by our boy EASports_MUT                                                   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:03:39.359000Z",
     "start_time": "2019-03-24T17:03:39.300026Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[['parent_comment', 'comment']]\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:04:11.098722Z",
     "start_time": "2019-03-24T17:03:44.239768Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "remove_punc = lambda x : re.sub(r\"\\W\", ' ', x)\n",
    "\n",
    "remove_extra_spaces = lambda x : re.sub(r\"\\s+\", ' ', x)\n",
    "\n",
    "lower_case = lambda x : x.lower()\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "remove_stopwords = lambda x: ' '.join(word for word in x.split() if word not in stop_words)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ps_stem = lambda x: ' '.join(ps.stem(word) for word in x.split())\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "wnl_lemmatize = lambda x: ' '.join(wnl.lemmatize(word) for word in x.split())\n",
    "\n",
    "def tag_pos(x):\n",
    "    tag_list =  nltk.pos_tag(nltk.word_tokenize(x))\n",
    "    pos = \"\"\n",
    "    for t in tag_list:\n",
    "        pos += t[0] +'(' + t[1] +')' + ' '\n",
    "    return pos\n",
    "\n",
    "def cleanText(x, rsw, stm, lem, tgps):\n",
    "    x = remove_punc(x)\n",
    "    x = remove_extra_spaces(x)\n",
    "    x = lower_case(x)\n",
    "    if rsw:\n",
    "        x = remove_stopwords(x)\n",
    "    if stm:\n",
    "        x = ps_stem(x)\n",
    "    if lem:\n",
    "        x = wnl_lemmatize(x)\n",
    "    if tgps:\n",
    "        x = tag_pos(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:08:22.032842Z",
     "start_time": "2019-03-24T17:04:15.197881Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Ritvik\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X['parent_comment'] = X['parent_comment'].apply(lambda x: cleanText(x, True, False, True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:10:21.771885Z",
     "start_time": "2019-03-24T17:08:22.268528Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Ritvik\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X['comment'] = X['comment'].apply(lambda x: cleanText(str(x), True, False, True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:10:22.013408Z",
     "start_time": "2019-03-24T17:10:21.989120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yeah get argument point prefer lived nc well</td>\n",
       "      <td>nc nh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blazer maverick west 5 6 seed even carry good enough record make playoff east last year</td>\n",
       "      <td>know west team play west team east team right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>favored win</td>\n",
       "      <td>underdog earlier today since gronk announcement afternoon vega line moved patriot 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deadass kill buzz</td>\n",
       "      <td>meme funny none new york nigga one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yep confirm saw tool use made boy easports_mut</td>\n",
       "      <td>could use one tool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            parent_comment  \\\n",
       "0  yeah get argument point prefer lived nc well                                              \n",
       "1  blazer maverick west 5 6 seed even carry good enough record make playoff east last year   \n",
       "2  favored win                                                                               \n",
       "3  deadass kill buzz                                                                         \n",
       "4  yep confirm saw tool use made boy easports_mut                                            \n",
       "\n",
       "                                                                               comment  \n",
       "0  nc nh                                                                                \n",
       "1  know west team play west team east team right                                        \n",
       "2  underdog earlier today since gronk announcement afternoon vega line moved patriot 1  \n",
       "3  meme funny none new york nigga one                                                   \n",
       "4  could use one tool                                                                   "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:10:22.242440Z",
     "start_time": "2019-03-24T17:10:22.234461Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:10:22.497758Z",
     "start_time": "2019-03-24T17:10:22.447893Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tvectorizer1 = TfidfVectorizer(ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:15:13.031821Z",
     "start_time": "2019-03-24T17:10:22.710190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tvectorizer1.fit(X['parent_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:16:44.498955Z",
     "start_time": "2019-03-24T17:15:17.460789Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xvect1 = Tvectorizer1.transform(X['parent_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:16:46.644446Z",
     "start_time": "2019-03-24T17:16:46.638463Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tvectorizer2 = TfidfVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:18:05.300585Z",
     "start_time": "2019-03-24T17:16:48.704938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tvectorizer2.fit(X['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:18:38.411495Z",
     "start_time": "2019-03-24T17:18:07.455825Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xvect2 = Tvectorizer2.transform(X['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:18:41.454189Z",
     "start_time": "2019-03-24T17:18:41.364968Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:18:48.897629Z",
     "start_time": "2019-03-24T17:18:44.283624Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xvect = hstack([Xvect1, Xvect2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:18:53.083426Z",
     "start_time": "2019-03-24T17:18:52.823135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010826, 21792523)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xvect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(Tvectorizer1, open('v2/Tvect1.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(Tvectorizer2, open('v2/Tvect2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:18:56.890666Z",
     "start_time": "2019-03-24T17:18:56.789775Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:19:00.402236Z",
     "start_time": "2019-03-24T17:19:00.223754Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:48:28.106957Z",
     "start_time": "2019-03-24T17:19:03.435132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "scores = cross_val_score(model, Xvect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' %(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:48:33.484825Z",
     "start_time": "2019-03-24T17:48:32.587315Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T17:49:51.684395Z",
     "start_time": "2019-03-24T17:48:36.592525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "scores = cross_val_score(model, Xvect, y, cv=10)\n",
    "print('Accuracy: %0.2f (+/- %0.2f)' %(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
